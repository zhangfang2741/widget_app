import streamlit as st
import yfinance as yf
import pandas as pd
import plotly.graph_objects as go
import os
import re
from google import genai
from google.genai import types
import logging
import contextlib

# Reduce noisy log output from yfinance / urllib3
logging.getLogger('yfinance').setLevel(logging.ERROR)
logging.getLogger('urllib3').setLevel(logging.WARNING)

# Helper to suppress noisy yfinance stdout/stderr (prevents 'Failed download' lines flooding the UI/console)
def safe_yf_download(*args, **kwargs):
    devnull = open(os.devnull, 'w')
    try:
        with contextlib.redirect_stdout(devnull), contextlib.redirect_stderr(devnull):
            return yf.download(*args, **kwargs)
    finally:
        devnull.close()


def sanitize_and_validate_user_tickers(raw_input, max_keep=50):
    """Parse and validate user-provided tickers. Returns (valid_list, invalid_list).

    Uses a conservative regex + a short yf check to ensure the symbol exists before proceeding.
    """
    if not raw_input or not raw_input.strip():
        return [], []

    tokens = re.split(r'[,\s]+', raw_input.strip())
    seen = set()
    valid = []
    invalid = []

    for tok in tokens:
        if not tok:
            continue
        t = tok.strip().upper().lstrip('$').strip(',')
        t = t.replace('.', '-')  # normalize BRK.B -> BRK-B
        if not re.match(r'^[A-Z][A-Z0-9\-]{0,5}$', t):
            invalid.append(tok)
            continue
        if t in seen:
            continue
        seen.add(t)
        # quick existence check
        try:
            df = safe_yf_download(t, period='5d', interval='1d', progress=False, auto_adjust=True)
            if df is not None and not df.empty:
                valid.append(t)
            else:
                invalid.append(tok)
        except Exception:
            invalid.append(tok)
        if len(valid) >= max_keep:
            break

    return valid, invalid

# --- 1. ç¯å¢ƒä¸è¿æ¥é…ç½® ---
os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:8118'
os.environ['HTTP_PROXY'] = 'http://127.0.0.1:8118'

API_KEY = "AIzaSyAHv7J2ukKTfMCrIXjFF-PE_fJdBBEzGZs"
client = genai.Client(api_key=API_KEY)


# --- 2. æ ¸å¿ƒé‡åŒ–ç®—æ³•åº“ ---
@st.cache_data(ttl=3600)
def get_structured_data(ticker):
    try:
        data = safe_yf_download(ticker, period="1y", interval="1d", auto_adjust=True)
        if data.empty: return None
        if isinstance(data.columns, pd.MultiIndex):
            data.columns = data.columns.get_level_values(0)

        close = data['Close']
        if isinstance(close, pd.DataFrame):
            close = close.iloc[:, 0]

        ma50 = close.rolling(50).mean()
        ma200 = close.rolling(200).mean()

        delta = close.diff()
        gain = (delta.where(delta > 0, 0)).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rsi = 100 - (100 / (1 + (gain / loss)))

        curr_p = float(close.iloc[-1])
        curr_m50 = float(ma50.iloc[-1])
        curr_m200 = float(ma200.iloc[-1])
        curr_rsi = float(rsi.iloc[-1])
        bias = (curr_p - curr_m200) / curr_m200

        if curr_p > curr_m200 and curr_m50 < curr_m200:
            phase = "ğŸŸ¢ èµ·æ­¥é˜¶æ®µ"
        elif curr_p > curr_m50 > curr_m200 and bias < 0.25:
            phase = "ğŸ“ˆ ä¸Šå‡é˜¶æ®µ"
        elif curr_p > curr_m50 and bias >= 0.25:
            phase = "ğŸ”¥ æˆç†Ÿé˜¶æ®µ"
        else:
            phase = "ğŸ“‰ è°ƒæ•´é˜¶æ®µ"

        return {"df": data, "price": curr_p, "rsi": curr_rsi, "bias": bias, "phase": phase, "ticker": ticker}
    except Exception as e:
        print(f"[{ticker}] æ•°æ®æŠ“å–å¤±è´¥: {e}")
        return None


# --- 3. å¼€å¯æœç´¢èƒ½åŠ›çš„ AI æ¨¡å— ---

@st.cache_data(ttl=3600)
def get_ai_suggestions():
    """è®© AI æ¨èå½“å‰çƒ­ç‚¹å¹¶å¯¹å€™é€‰ä»£ç è¿›è¡Œä¸¥æ ¼æ ¡éªŒåè¿”å›

    Steps:
    - Use a tighter regex to extract plausible ticker-like tokens (letters, and optional . or - parts)
    - Filter out common English stopwords / noise tokens
    - Verify each candidate with a short yfinance download (5d) and keep only those with data
    - Return a deduplicated, comma-separated list (max 12)
    """
    time_str = pd.Timestamp.now().strftime("%Yå¹´-%mæœˆ-%dæ—¥")
    prompt = f"""
    è¯·åœ¨ç¾è‚¡å¸‚åœºä¸­ç­›é€‰å‡ºå½“å‰ï¼ˆ{time_str}ï¼‰æ—¶é—´æœ€è¿‘ 5 ä¸ªäº¤æ˜“æ—¥å†…æ»¡è¶³ä»¥ä¸‹é‡ä»·ä¸æƒ…ç»ªç‰¹å¾çš„ 6â€“8 ä¸ªETFï¼ˆä¸è¦è‚¡ç¥¨ï¼‰ï¼š
    1) æˆäº¤é‡è¿ç»­æ”¾å¤§ï¼ˆè¿ç»­ 3 æ—¥æˆ–ä»¥ä¸Šæˆäº¤é‡ç¯æ¯”ä¸Šå‡ï¼‰
    2) ä»·æ ¼è¿‘æœŸåˆ›è¿‘æœŸé«˜ç‚¹æˆ–å‘ˆç¨³æ­¥æ”€å‡è¶‹åŠ¿
    3) ç¤¾äº¤/æ–°é—»æƒ…ç»ªæ˜æ˜¾ä¸Šå‡ï¼ˆå¦‚æƒ…ç»ªæ•°æ®æˆ–ç¤¾åª’çƒ­åº¦/æåŠåº¦æ˜æ˜¾æé«˜ï¼‰

    è¯·ä¼˜å…ˆæ¶µç›–ä»¥ä¸‹ä¸»é¢˜ï¼š
    - ç§‘æŠ€/AI 
    - åŠ å¯†è´§å¸
    - è´µé‡‘å±ï¼ˆé»„é‡‘/ç™½é“¶ï¼‰
    - èƒ½æº
    - åŠ¨é‡/æƒ…ç»ªå‹

    ä»…è¿”å›ä»£ç æˆ–ç¬¦å·ï¼Œç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚ä¾‹å¦‚ï¼šQQQ, NVDA, GLD
    """
    response = client.models.generate_content(
        model='gemini-2.0-flash', contents=prompt
    )
    raw_text = response.text.upper()
    # æ›´ä¸¥æ ¼çš„æ­£åˆ™ï¼šå…è®¸å½¢å¦‚ NVDA æˆ– BRK.B æˆ– RIVN ç­‰ï¼ˆå­—æ¯ 1-5ï¼Œå¯é€‰ . æˆ– - åç¼€ï¼‰
    candidates = re.findall(r"\b[A-Z]{1,5}(?:[.\-][A-Z0-9]{1,2})?\b", raw_text)

    # é»‘åå• / åœç”¨è¯ï¼Œæ‰©å±•ä¸€äº›å¸¸è§å™ªå£°è¯
    noise = {'ETF', 'AI', 'USD', 'THE', 'AND', 'WITH', 'YOUR', 'ALSO', 'COULD', 'MAY', 'WILL', 'FOR',
             'NOT', 'THIS', 'THAT', 'FROM', 'HAS', 'HAVE', 'IN', 'ON', 'AT', 'BY', 'ABOUT', 'OVER'}

    # ä¿ç•™å€™é€‰å¹¶å»é‡ï¼Œä¸”æ’é™¤çº¯æ•°å­—æˆ–é•¿åº¦è¿‡é•¿çš„token
    seen = set()
    filtered = []
    for tok in candidates:
        if tok in noise:
            continue
        # skip tokens that look like months or common words
        if re.match(r'^[A-Z]{1,2}$', tok) and tok not in ['XL', 'XLU', 'XLK', 'SP', 'BR']:
            # 1-2 letter tokens are rarely valid tickers on US exchanges; let yf validate them later
            pass
        # normalize BRK.B -> BRK-B for yfinance compatibility
        norm = tok.replace('.', '-')
        if norm in seen:
            continue
        seen.add(norm)
        filtered.append(norm)

    # Validate with yfinance to remove delisted/invalid symbols
    valid = []
    for sym in filtered:
        try:
            # try a very short download to check existence
            dd = safe_yf_download(sym, period='5d', interval='1d', progress=False, auto_adjust=True)
            if not dd.empty:
                valid.append(sym)
        except Exception:
            # ignore symbols that yfinance can't fetch
            continue
        # limit to reasonable number to avoid long loops
        if len(valid) >= 12:
            break

    if not valid:
        return "QQQ, NVDA, SMH, GLD, SLV, BITO, COIN"

    return ",".join(valid)


@st.cache_data(ttl=3600)
def get_ai_summary(ticker, phase, rsi, bias):
    """åŸºäºå®æ—¶æœç´¢çš„æ·±åº¦ç»“æ„åŒ–é¢„æµ‹"""
    search_tool = types.Tool(google_search=types.GoogleSearch())

    # ç³»ç»ŸæŒ‡ä»¤ï¼šå¼ºåˆ¶æ¨¡å‹è¿›è¡Œç»“æ„åŒ–é“¾å¼æ¨ç†
    sys_instruction = "ä½ æ˜¯ä¸€ä¸ªå…¨çƒå®è§‚ç­–ç•¥ä¸“å®¶ã€‚ä½ å¿…é¡»ç»“åˆ Google æœç´¢åˆ°çš„ 2026 å¹´æœ€æ–°å®è§‚æ•°æ®ã€æ”¿ç­–å˜é‡è¿›è¡Œæ¨ç†ã€‚"

    prompt = f"""
    åˆ†æETFï¼š{ticker}ã€‚å½“å‰æŠ€æœ¯æŒ‡æ ‡ï¼šé˜¶æ®µ={phase}, RSI={rsi:.1f}, ä¹–ç¦»ç‡(Bias)={bias:.1%}ã€‚

    ä»»åŠ¡ï¼š
    1. æœç´¢å¹¶ç¡®è®¤è¯¥èµ„äº§è¿‘æœŸçš„æ ¸å¿ƒé©±åŠ¨äº‹ä»¶ï¼ˆå¦‚è´¢æŠ¥ã€åˆ©ç‡å†³è®®ã€åœ°ç¼˜åŠ¨æ€ã€éœ€æ±‚çƒ­åº¦ç­‰ï¼‰ã€‚
    2. åŸºäºâ€œç»“æ„åŒ–ä¼ å¯¼é€»è¾‘â€ï¼šå¦‚æœè¯¥èµ„äº§æŒç»­èµ°å¼º/èµ°å¼±ï¼Œå“ªä¸€ä¸ªå…³è”äº§ä¸šç¯èŠ‚å°†æˆä¸ºä¸‹ä¸€ä¸ªçˆ†å‘ç‚¹ï¼Ÿ
    3. ç»™å‡ºé¢„æµ‹ï¼šè¶‹åŠ¿ â†’ ä¾›åº”é“¾/æµåŠ¨æ€§å˜åŒ– â†’ èµ„äº§å½¢æ€ã€‚
    è¦æ±‚ï¼šç›´æ¥ç»™å‡º 2 ä¸ªç²¾å‡†çš„ä¼ å¯¼æ–¹å‘å’Œæœ€å¯èƒ½çˆ†å‘çš„èµ„äº§ä»£ç ï¼Œ200å­—ä»¥å†…ï¼Œä¸è¦åºŸè¯ã€‚
    """
    try:
        response = client.models.generate_content(
            model='gemini-2.0-flash',
            contents=prompt,
            config=types.GenerateContentConfig(
                system_instruction=sys_instruction,
                tools=[search_tool]
            )
        )
        return response.text.strip()
    except Exception as e:
        return f"AI æ¨ç†æš‚æ—¶å—é˜»: {str(e)}"


# --- 4. UI è¾…åŠ©å‡½æ•° ---

def make_sparkline(data_series):
    colors = ['#2ca02c' if data_series.iloc[-1] >= data_series.iloc[0] else '#ff4b4b']
    # Correct color comparison and choose fill color based on the actual color
    primary = colors[0]
    if primary == '#2ca02c':
        fill = 'rgba(44, 160, 44, 0.1)'
    else:
        fill = 'rgba(255, 75, 75, 0.1)'

    fig = go.Figure(data=go.Scatter(
        y=data_series, mode='lines', line=dict(color=primary, width=2),
        fill='tozeroy', fillcolor=fill
    ))
    fig.update_layout(
        showlegend=False, xaxis=dict(visible=False), yaxis=dict(visible=False),
        margin=dict(t=5, b=5, l=0, r=0), height=40,
        paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)'
    )
    return fig


def get_top_stocks_in_etf(etf_ticker):
    """Fetch the top-performing stocks within a given ETF."""
    try:
        # Download ETF holdings data (mocked for demonstration)
        holdings = safe_yf_download(etf_ticker, period='1y', interval='1d', progress=False, auto_adjust=True)
        if holdings.empty:
            return f"No data available for ETF: {etf_ticker}"

        # Analyze holdings to find top-performing stocks
        top_stocks = holdings.sort_values(by='performance_metric', ascending=False).head(5)
        return top_stocks[['ticker', 'performance_metric']]
    except Exception as e:
        return f"Error fetching data for ETF {etf_ticker}: {str(e)}"


# --- 5. Streamlit é¡µé¢æ„å»º ---
st.set_page_config(page_title="US Asset Structural Trends 2026", layout="wide")

st.title("ğŸ›¡ï¸ æ ¸å¿ƒèµ„äº§ç»“æ„åŒ–è¶‹åŠ¿çœ‹æ¿ (AI æœç´¢å¢å¼ºç‰ˆ)")

# åˆå§‹åŒ– Ticker åˆ—è¡¨
if 'user_tickers' not in st.session_state:
    st.session_state.user_tickers = "IBIT, QQQ, GLD, SLV, SMH"


def auto_detect_market():
    # Correcting the usage of st.spinner to ensure it works as a context manager
    with st.spinner("AI æ­£åœ¨å…¨ç½‘æœç´¢æœ€æ–°å™äº‹ç„¦ç‚¹..."):
        suggestions = get_ai_suggestions()
        st.session_state.user_tickers = suggestions


with st.sidebar:
    st.header("âš™ï¸ åŠ¨æ€èµ„äº§é…ç½®")
    st.button("ğŸ¤– AI æ‰«æä»Šæ—¥å…¨çƒçƒ­ç‚¹", on_click=auto_detect_market)
    ticker_input = st.text_area("æ ‡çš„åˆ—è¡¨ (é€—å·åˆ†éš”)", key="user_tickers", height=100)

tickers = [t.strip().upper() for t in ticker_input.split(",") if t.strip()]

# æ•°æ®å‡†å¤‡é€»è¾‘
asset_map = {"IBIT": "æ¯”ç‰¹å¸ç°è´§", "QQQ": "çº³æŒ‡100", "GLD": "é»„é‡‘ç°è´§", "SLV": "ç™½é“¶ç°è´§", "SMH": "åŠå¯¼ä½“"}

# Adding a column to display the Chinese name of each ETF
# Assuming we have a dictionary mapping ETF codes to their Chinese names
ETF_CHINESE_NAMES = {
    "QQQ": "çº³æ–¯è¾¾å…‹100æŒ‡æ•°",
    "NVDA": "è‹±ä¼Ÿè¾¾",
    "SMCI": "è¶…çº§å¾®ç”µè„‘",
    "IBIT": "æ¯”ç‰¹å¸æœŸè´§",
    "GLD": "é»„é‡‘ETF",
    "GDX": "é»„é‡‘çŸ¿ä¸šETF",
    "XOM": "åŸƒå…‹æ£®ç¾å­š",
    "ARKK": "æ–¹èˆŸåˆ›æ–°ETF"
}

# UI æ¸²æŸ“å±‚
table_data = []
progress_text = "æ­£åœ¨æ‰§è¡Œç»“æ„åŒ–é‡åŒ–æ‰«æ..."
my_bar = st.progress(0, text=progress_text)

# Validate user input before processing
valid_tickers, invalid_tickers = sanitize_and_validate_user_tickers(st.session_state.user_tickers)
if invalid_tickers:
    st.warning(f"ä»¥ä¸‹æ ‡çš„æ— æ•ˆæˆ–æ•°æ®è·å–å¤±è´¥ï¼Œå°†è¢«å¿½ç•¥ï¼š{', '.join(invalid_tickers)}")

for idx, ticker in enumerate(valid_tickers):
    res = get_structured_data(ticker)
    if res:
        ai_pred = get_ai_summary(ticker, res['phase'], res['rsi'], res['bias'])
        table_data.append({**res, "ai_pred": ai_pred, "name": asset_map.get(ticker, ticker), "chinese_name": ETF_CHINESE_NAMES.get(ticker, "")})
    my_bar.progress((idx + 1) / len(valid_tickers), text=progress_text)
my_bar.empty()

# --- æ¸²æŸ“è¡¨æ ¼ ---
if table_data:

    st.markdown("---")
    header_cols = st.columns([0.6, 0.8, 1.2, 0.6, 1.2, 1.5, 3])

    labels = ["ä»£ç ", "èµ„äº§", "å½“å‰é˜¶æ®µ", "RSI", "ç»“æ„åŒ–ä¹–ç¦»åº¦", "12Mè¶‹åŠ¿", "ğŸ”® AI å®æ—¶é“¾å¼é¢„æµ‹"]

    # æ˜ç¡®è§£é‡Šæ¯ä¸€åˆ—æ•°å­—/å«ä¹‰ï¼Œä½¿ç”¨ caption åœ¨æ ‡é¢˜ä¸‹æ–¹å±•ç¤ºè¯´æ˜
    header_explanations = [
        "æ ‡çš„ä»£ç ï¼ˆTickerï¼‰ï¼Œæ”¯æŒ ETF æˆ– å•åªè‚¡ç¥¨ï¼Œä¾‹å¦‚ QQQã€NVDA",
        "åº•å±‚èµ„äº§æˆ–ä¸»é¢˜çš„ç®€çŸ­è¯´æ˜ï¼Œä¾‹å¦‚ï¼šçº³æŒ‡100 / è‹±ä¼Ÿè¾¾ / é»„é‡‘",
        "åŸºäºå‡çº¿ä¸ä¹–ç¦»ç‡åˆ¤å®šçš„å‘¨æœŸé˜¶æ®µï¼šèµ·æ­¥ / ä¸Šå‡ / æˆç†Ÿ / è°ƒæ•´ï¼ˆå¸¦å›¾æ ‡æç¤ºï¼‰",
        "ç›¸å¯¹å¼ºå¼±æŒ‡æ•° (RSI, 0-100)ï¼Œ>70 ä¸ºè¶…ä¹°ï¼ˆçŸ­æœŸå›è°ƒé£é™©ï¼‰ï¼Œ<30 ä¸ºè¶…å–ï¼ˆå¯èƒ½åå¼¹ï¼‰",
        "ç»“æ„åŒ–ä¹–ç¦»åº¦ = (å½“å‰ä»· - 200æ—¥å‡çº¿) / 200æ—¥å‡çº¿ï¼›\n>25%: è¿‡çƒ­ï¼ˆå†å²é«˜é£é™©ï¼‰ï¼Œ0~25%: å¥åº·ä¸Šå‡ï¼Œ<0%: å›è°ƒ/ä½ä¼°",
        "è¿‡å» 12 ä¸ªæœˆçš„ä»·æ ¼è¶‹åŠ¿ç¼©ç•¥å›¾ï¼ˆç»¿è‰²ä¸Šå‡ / çº¢è‰²ä¸‹é™ï¼‰",
        "AI æ ¹æ®ç»“æ„åŒ–ä¼ å¯¼é€»è¾‘ç»™å‡ºçš„ 3-12 ä¸ªæœˆæ½œåœ¨çˆ†å‘æ–¹å‘ï¼›æ”¯æŒ Markdownï¼Œå†…å®¹é«˜åº¦è‡ªé€‚é…å¹¶å¯æ»šåŠ¨æŸ¥çœ‹"
    ]

    for col, label, help_text in zip(header_cols, labels, header_explanations):
        col.markdown(f"**{label}**")
        # caption è¾ƒå°å­—ä½“æ˜¾ç¤ºè§£é‡Šï¼Œä¾¿äºç”¨æˆ·ç›´æ¥é˜…è¯»è¡¨å¤´å«ä¹‰
        col.caption(help_text)
    st.markdown("---")

    for row in table_data:
        c1, c2, c3, c4, c5, c6, c7 = st.columns([0.6, 0.8, 1.2, 0.6, 1.2, 1.5, 3])
        c1.markdown(f"#### {row['ticker']}")
        c2.caption(row['name'])
        c3.caption(row['phase'])

        rsi_val = row['rsi']
        c4.markdown(f":{'red' if rsi_val > 70 else 'blue' if rsi_val < 30 else 'green'}[**{rsi_val:.0f}**]")

        bias_pct = row['bias'] * 100
        bar_color = "#ff4b4b" if row['bias'] >= 0.25 else "#2ca02c" if row['bias'] >= 0 else "#1f77b4"
        c5.markdown(
            f"""<div style="background-color: {bar_color}22; border-radius: 4px; padding: 2px 8px;"><span style="color: {bar_color}; font-weight: bold;">{bias_pct:+.1f}%</span></div>""",
            unsafe_allow_html=True)

        fig = make_sparkline(row['df']['Close'])
        c6.plotly_chart(fig, use_container_width=True, config={'displayModeBar': False}, key=f"chart_{row['ticker']}")

        with c7.container(height=150, border=False):
            st.markdown(row['ai_pred'])
        st.divider()

    st.caption("æ³¨ï¼šAI é¢„æµ‹åŸºäº Google Search å®æ—¶æ£€ç´¢ã€‚é‡åŒ–æŒ‡æ ‡æ¯å°æ—¶æ›´æ–°ã€‚")
else:
    st.info("è¯·åœ¨å·¦ä¾§æ·»åŠ æ ‡çš„æˆ–ç‚¹å‡» AI æ‰«æã€‚")